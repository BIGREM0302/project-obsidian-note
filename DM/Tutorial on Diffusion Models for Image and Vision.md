[[Diffusion Model Basic]]
[[KL Divergence]]

This note avoid complex math derivation for simplicity and quick overview
## 1 Variational Auto-Encoder

### 1.1 Building blocks of VAE
Aim: We want to build a generator that generates images from some inputs.

Building block: Input x -> encoder -> latent variable z -> decoder -> generated x

where the latent variable z should extract some valuable information from the input x, and at the same time serves as the seed to generate something $\sim$ x

There are a lot of distributions involved, we should clarify them and figure out what is "known" and what is "unknown", what is to be trained.

- $p(x)$ : The *true* distribution of x, and is NEVER KNOWN.
- $p(z)$ : Typically we *made* it a zero mean unit variance Gaussian "for mathematical convenience"
- $p(z|x)$ : conditional distribution associated with encoder, but we can NEVER ACESS
- $p(x|z)$ : conditional distribution associated with decoder, but we can NEVER ACESS

To find encoder and decoder, we have to determine:
- $q_\phi(z|x)$ : Decoder, can be parameterized using deep NN, and can be any directed graphical model, to approximate p(z|x)
- $p_\theta(x|z)$ : Encoder, to approximate p(x|z)
### 1.2 Evidence Lower Bound

We'd like to find the best theta and phi, so we need an objective function(loss function):$$ELBO(x)\equiv\mathbb{E}_{q_\phi(z|x)}[\log \frac{p(x, z)}{q_\phi(z|x)}]$$Meaning: given x,  the expectatation value of log(p(x, z)/q_phi(z|x)) subject to z ~ q_phi(z|x)
which is the LOWER BOUND for the distribution logp(x) $$\log p(x)=ELBO(x)+KL(q_\phi(z|x)||p(z|x))$$Remember that we want to approximate p(z|x) by using q_phi(z|x)? That is, since logp(x) is a constant (which is generated by the real word, or the universe), then by maximizing ELBO, we can minimize KL divergence between q_phi(z|x) and p(z|x) as the result.
Further we can divide ELBO to the following.$$ELBO(x)=\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]-KL(q_\phi(z|x)||p(z))$$We want to maximize ELBO, so maximize logp_theta, and minimize the difference between p(z) and q_phi(z|x). We can further recognize these two terms as:
- **Reconstruction**: first term is about the decoder, since we are maximize the expectation of logp_theta (the encoding process) subject to z given x. That is, subject to q_theta(encode process) we'd like to increase the likelihood of the reconstruction toward the original x.
- **Prior Matching**: We want the encoder to turn x into latent vector z s.t. it will follow "our choice of distribution" e.g. z~N(0, I)
### 1.3 Optimization in VAE
$$(\phi,\theta) = argmax_{\phi,\theta}\sum_{x\in\mathcal{X}}ELBO(x);\ where\ \mathcal{X}=\{x^{(l)}|l=1,\dots,L\}$$X is the training set with size L. However we find that it's hard to solve if we directly calculate gradient:$$\nabla_\theta ELBO(x)\ and\ \nabla_\phi ELBO(x)$$Even though we can write: *note that the numerator of ELBO here is p_theta instead of real p*
$$\nabla_\theta ELBO(x)\approx \frac{1}{L}\sum_{l=1}^{L}\nabla_\theta\{\log p_\theta(x,z^{(l)})\} :where\ z^{(l)}\sim q_\phi(z|x)$$The approximation is correlated with [[Monte Carlo Approximation]]. Since p_theta is realized by a computable model such as NN, the gradient can be thus computed via automatic differntiation and update the parameter by back propogating. However, we fail to derive the similar representation for parameter phi, the REASON is that we have to draw samples z from distribution q_phi(z|x) which is also dependent of phi!
#### Reparameterization Trick
Aim to tackle with the intractability of ELBO's gradient. i.e. to express z in the following way$$z=g(\epsilon,\phi,x)$$for the invertible and differentiable transformation g and another random varaible epsilon whose distribution is INDEPENDENT of x and phi.

An addtional requirement: $$q_\phi(z|x)\cdot |det(\frac{\partial z}{\partial \epsilon})|=p(\epsilon)\text{ where }\frac{\partial z}{\partial \epsilon}:=
\begin{bmatrix}
\frac{\partial z_1}{\partial \epsilon_1}&\dots&\frac{\partial z_1}{\partial \epsilon_n}\\
\vdots & & \vdots\\
\frac{\partial z_n}{\partial \epsilon_1}&\dots&\frac{\partial z_n}{\partial \epsilon_n}
\end{bmatrix}:Jacobian\ matrix$$With this re-parameterization of z, we can write:$$\mathbb{E}_{q_\phi(z|x)}[f(z)]=\dots=\mathbb{E}_{p(\epsilon)}[f(z)]$$and therefore we can further calculate the gradient:$$\nabla_\phi ELBO(x)\approx\frac{1}{L}\sum_{l=1}^L\nabla_\phi[\log|det\frac{\partial z^{(l)}}{\partial \epsilon^{(l)}}|]$$Similarly, the approximation is correlated to Monte Carlo approximation. Now both gradients we can calculate thanks to this trick and also the approximation, which can lead to unbiased estimate when maximizing ELBO.
#### VAE Encoder
ASSUME a COMMON CHOICE of encoder: The Gaussian Distribution$$q_\phi(z|x)=\mathcal{N}(z|\mu_\phi(x),\sigma_\phi(x)^2I)$$Then we can use the neural network to determine the mean and variance of the Gaussian given input, and then sample latent z from the distribution q_phi. A more convenient way to express z is to adopt the REPARAMETERIZATION trick:$$z^{(l)}|x^{(l)}=\mu_\phi(x^{(l)})+\sigma_\phi(x^{(l)})\epsilon,\ \epsilon\sim\mathcal{N}(0,I)$$Notice that latent variable z is still p(z) = N(0,I). The epsilon is also a gaussian, it's consistent with the additional requirement elaborated above: $q_\phi(z|x)\cdot |det(\frac{\partial z}{\partial \epsilon})|=p(\epsilon)$ 
Let's substitute the q_phi derived above into the prior matching term of ELBO: $$\nabla\phi\bigg( KL\big(q_\phi(z|x)||p(z)\big)\bigg)=\nabla_\phi\bigg(something\ related\ to\ \phi\ instead\ of\ \theta\bigg)$$It's derived by using the formula for KL divergence for two gaussian distributions, and a closed form of the gradient can be numerically solved. 

#### VAE Decoder
ASSUME a COMMON CHOICE of decoder: The Gaussisan Distribution$$p_\theta(x|z)=\mathcal{N}(x|f_\theta(z),\sigma_{dec}^2I):\sigma_{dec}\text{ are some hyperparameter}$$Estimate the mean of Guassisan by input latenze vector z to a NN. Similarly, by reparameterization trick the generated x can be written as $\hat{x}=f_\theta(z)+\sigma_{dec}\epsilon,\ \epsilon\sim\mathcal{N}(0,I)$. 
Then, we can now calculate the Reconstruction term of ELBO:$$\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]=\int \dots\approx-\frac{1}{M}\sum_{m=1}^{M}\frac{||x-f_\theta(z^{(m)})||^2}{2\sigma^2_{dec}}=-\frac{1}{M}\sum_{m=1}^{M}\frac{||x-f_\theta\big(\mu_\phi(x)+\sigma_\phi(x)\epsilon^{(m)}\big)||^2}{2\sigma^2_{dec}}$$The approximation is again from Monte Carlo. Note that M is the number of Monte Carlo samples we want to use to estimate the expectation, where the randomness is achieved by $\epsilon\sim\mathcal{N}(0.I)$
Note x is fixed since the whole equation is under a given x!

Gradient of this term respect to theta is easily to compute since we know f_theta.. Respect to phi can also be calculated by chain rule. We can still use gradient descent to train NN.

### 1.4 Conclusion
To train VAE = to optimize theta and phi by runing *stochastic* gradient descent$$argmax_{\theta,\phi}ELBO_{\phi,\theta}(x)$$$$ELBO_{\phi,\theta}=-\frac{1}{M}\sum_{m=1}^{M}\frac{||x-f_\theta\big(\mu_\phi(x)+\sigma_\phi(x)\epsilon^{(m)}\big)||^2}{2\sigma^2_{dec}}+\frac{1}{2}\bigg(\sigma^2_\phi(x)d-d+||\mu_\phi(x)||^2-2d\log\sigma_\phi(x)\bigg)$$
where d is the dimension of latent variable z

VAE Inference = generate image by randomly pick $z\in\mathbb{R}^d$ , send z through decoder f_theta and then generate x = f_theta(z)

## 2. DDPM

### 2.1 Building Blocks

#### Initial Block:
Focus on state x_0:  Use *Gaussian* $p_\theta(x_0|x_1)$ to approximate $p(x_0|x_1)$

#### Transition Block:
Focus on state x_t: Use Gaussian $p_\theta(x_t|x_{t+1})$ to approximate $p(x_t|x_{t+1})$, and use Gaussian $q_{\phi}(x_t|x_{t-1})$ to approximate $p(x_t|x_{t-1})$

#### Final Block:
Focus on state x_T: Use Gaussian $q_\phi(x_T|x_{T-1})$ to approximate $p(x_T|x_{T-1})$ 


**For the forward process we can define**$$q_\phi(x_t|x_{t-1})=\mathcal{N}(\sqrt{\alpha_t}x_{t-1}, (1-\alpha_t)\mathbf{I})$$which implies that $$x_t=\sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}\epsilon,\ where\ \epsilon \sim \mathcal{N}(0,\mathbf{I})$$There are reasons why choose the coefficient in this way:
1. This coefficient arrangement can show that if x_0 is a Gaussian mixture model, then when t sufficiently large, x_t will become a WHITE GAUSSIAN NOISE: N(0,I)
2. When start from x_t = a * x_t-1 + b * epsilon_t-1, where epsilon_t are i.i.d. N(0,I), then when t is very large, we have to make x_t's Covariance matrix = I, and then we'll find $1=\frac{b^2}{1-a^2}$
And then we can derive $$q_\phi(x_t|x_0)=\mathcal{N}(\sqrt{\bar{\alpha_t}}x_0, (1-\bar{\alpha_t})\mathbf{I})\ where\ \bar{\alpha_t}=\prod_{i=1}^{t}\alpha_i$$
### 2.2 ELBO
$$\log p(x) = \log p(x_o) = \dots \geq \mathbb{E}_{q_\phi(x_{1:T}|x_0)}\big[\log\frac{p(x_{0:T})}{q_\phi(x_{1:T}|x_0)}\big]$$We use Jensen's inequality, which states that $f(E[X])\geq E[f(X)]$ where f is any concave function.
With further derivation, we get$$\log p(x)\geq\mathbb{E}_{q_\phi(x_{1:T}|x_0)}\big[\log p(x_0|x_1)\big]+\mathbb{E}_{q_\phi(x_{1:T}|x_0)}\big[\log\frac{p(x_T)}{q_\phi(x_T|x_{T-1})}\big]+\mathbb{E}_{q_\phi(x_{1:T}|x_0)}\bigg[\log\prod_{t=1}^{T-1}\frac{p(x_t|x_{t+1})}{q_\phi(x_t|x_{t-1})}\bigg]$$The first term is reconstruction term, the second term is prior matching, and the last one is consistency term.

We can rewrite them and produce a simpler format:
- Reconstruction Term becomes $$\mathbb{E}_{q_\phi(x_1|x_0)}\big[\log p_\theta(x_0|x_1)\big]$$
- Prior matching Term:$$\mathbb{E}_{q_\phi(x_{1:T}|x_0)}\big[\log\frac{p(x_T)}{q_\phi(x_T|x_{T-1})}\big]=\mathbb{E}_{q_\phi(x_T,x_{T-1}|x_0)}\big[\log\frac{p(x_T)}{q_\phi(x_T|x_{T-1})}\big]=-\mathbb{E}_{q_\phi(x_{T-1}|x_0)}\big[KL(q_\phi(x_T|x_{T-1})||p(x_T))\big]$$The above derivation need use the chain rule for q_phi and also the Markovian property of q_phi.
- Consistency Term:$$\mathbb{E}_{q_\phi(x_{1:T}|x_0)}\bigg[\log\prod_{t=1}^{T-1}\frac{p(x_t|x_{t+1})}{q_\phi(x_t|x_{t-1})}\bigg]=\dots = -\sum_{t=1}^{T-1}\mathbb{E}_{q_\phi(x_{t-1},x_{t+1}|x_0)}\bigg[KL(q_\phi(x_t|x_{t-1})||p_\theta(x_t|x_{t+1}))\bigg]$$
However, with different derivation way, we can also derive:$$ELGO_{\phi,\theta}(x)=\mathbb{E}_{q_\phi(x_1|x_0)}\big[\log p_\theta(x_0|x_1)\big]-KL(q_\phi(x_T|x_0)||p(x_T))-\sum_{t=2}^T\mathbb{E}_{q_\phi(x_t|x_0)}\big[KL(q_\phi(x_{t-1}|x_t,x_0)||p_\theta(x_{t-1}|x_t))\big]$$With the reconstruction term unchanged, we get new prior matching and consistency term with some algebraic mathematics.

### 2.3 Distribution of the Reverse Process
#### Discussion of $q_\phi(x_{t-1}|x_t, x_0)$ 
We can show that it IS A GAUSSIAN, where $$\mu_q(x_t, x_0)=\frac{(1-\bar{\alpha}_{t-1})\sqrt{\alpha_t}}{1-\bar{\alpha_t}}x_t+\frac{(1-\alpha_t)\sqrt{\bar{\alpha}_{t-1}}}{1-\bar{\alpha_t}}x_0\ \ ;\ \Sigma_q(t)=\frac{(1-\alpha_t)(1-\sqrt{\bar{\alpha}_{t-1}})}{1-\bar{\alpha_t}}\mathbf{I}\equiv\sigma_q^2\mathbf{I}$$It can by proved by Bayes theorem and do some constant matching (mean and covariance matrix). 
Therefore, it is actually fully decided by x_t and x_0 without needing any NN to estimate the mean and variance => NOTHING TO LEARN!

#### Discussion of $p_\theta(x_{t-1}|x_t)$
For simplicity, we CHOOSE IT AS GAUSSIAN. Denote its mean as $\mu_\theta(x_t)$ , while we use the same covariance matrix above: $\sigma_q^2\mathbf{I}$

Since, the KL divergence in the consistency term can be re-written in a simpler form:$$\frac{1}{2\sigma_q^2(t)}||\mu_q(x_t, x_0)-\mu_\theta(x_t)||^2$$In addition, the prior matching term is entirely no need to learn.

#### Discussion of $\log p_\theta(x_0|x_1)$
$$\log p_\theta(x_0|x_1)=-\frac{||x_0-\mu_\theta(x_1)||^2}{2\sigma_1^2(1)}-\frac{d}{2}\log(2\pi\sigma_q^2(1))$$

### 2.4 Training and Inference
Finally, how to train our baby model DDPM??
Somehow, by examining the definition of ELBO, it seems that we are always try to minimize the following loss function: $$\frac{1}{2\sigma_q^2(t)}||\mu_q(x_t,x_0)-\mu_\theta(x_t)||^2$$We can DESIGN $\mu_\theta$ in another way, just like the format of $\mu_q$:$$\mu_\theta(x_t)\equiv\frac{(1-\bar{\alpha}_{t-1})\sqrt{\alpha_t}}{1-\bar{\alpha_t}}x_t+\frac{(1-\alpha_t)\sqrt{\bar{\alpha}_{t-1}}}{1-\bar{\alpha_t}}\hat{x}_\theta(x_t)$$Substitute this new representation into the above loss function, then we can find that the ELBO can be written below:$$ELBO_\theta(x)=\mathbb{E}_{q_\phi(x_1|x_0)}\big[\log p_\theta(x_0|x_1)\big]-\sum_{t=2}^T\mathbb{E}_{q(x_t|x_0)}\bigg[\dots||\hat{x}_\theta(x_t)-x_0||^2\bigg]$$Where we drop the term prior matching term since it doesn't the need of training.
Then we can further absorb the reconstruction term into the summation:$$ELBO_\theta(x)=-\sum_{t=1}^T\dots\mathbb{E}_{q(x_t|x_0)}\big[||\hat{x}_\theta(x_t)-x_0||^2\big]$$IF we ignore the constant and expectation, then the main subject of interest is:$$argmin_{\theta}||\hat{x}_\theta(x_t)-x_0||^2$$Which is the very DENOISING problem: find a network $\hat{x}_\theta(x_t)$ which should be as close to the original image $x_0$. But recall we also have to consider: 
* $\mathbb{E}_{q(x_t|x_0)}$ : We are trying to denoise only for $x_t\sim q(x_t|x_0)$
* The ignored constant(t): We do not weight the denoising loss equally for all steps
#### The resulting model:
$$
argmax_\theta\sum_{x_0\in\mathcal{X}}ELBO(x_0)=argmin_\theta\sum_{x_0\in\mathcal{X}}\sum_{t=1}^T\mathbb{E}_{q(x_t|x_0)}\bigg[const(t)\cdot||\hat{x}_\theta(\sqrt{\bar{\alpha}_t}x_0+\sqrt{(1-\bar{\alpha_t})}\epsilon_t)-x_0||^2\bigg]$$
$$=argmin_\theta\sum_{x_0\in\mathcal{X}}\sum_{t=1}^T\frac{1}{M}\sum_{m=1}^Mconst(t)\cdot||\hat{x}_\theta(\sqrt{\bar{\alpha}_t}x_0+\sqrt{(1-\bar{\alpha_t})}\epsilon_t)-x_0||^2$$$$where\ const(t)\ =\ \frac{1}{2\sigma_q^2(t)}\frac{(1-\alpha_t)^2\bar{\alpha}_{t-1}}{(1-\bar{\alpha_t})^2}$$Forward Diffusion in DDPM$$x_t\sim q(x_t|x_0)=\mathcal{N}(x_t|\sqrt{\bar{\alpha}_t}x_0,(1-\bar{\alpha_t})\mathbf{I}),\ t=1,\dots,T-1$$Training Algorithm:
- Repeat the following steps until convergence.
- Pick a random time stamp $t\sim Uniform[1,T]$.
- Draw a sample $x_t^{(m)}=\bar{\alpha}_tx_0+\sqrt{(1-\bar{\alpha_t})}\epsilon_t^{(m)}$, $\epsilon_t^{(m)}\sim\mathcal{N}(0,\mathbf{I})$
- Take gradient descent step on$$\nabla_\theta\bigg\{\frac{1}{M}\sum_{m=1}^M||\hat{x}_\theta(x_t^{(m)})-x_0||^2\bigg\}$$
#### Inference of DDPM - the Reverse Diffusion
- Give us a white noise vector $x_T\sim\mathcal{N}(0,I).$ 
- Repeat the following for $t=T,T-1,\dots,1$.
- We calculate $\hat{x}_\theta(x_t)$ using our trained denoiser.
- Update according to $$x_{t-1}=\frac{(1-\bar{\alpha}_{t-1})\sqrt{\alpha_t}}{1-\bar{\alpha}_t}x_t+\frac{(1-\alpha_t)\sqrt{\bar{\alpha}_{t-1}}}{1-\bar{\alpha}_t}\hat{x}_\theta(x_t)+\sigma_q(t)\epsilon,\ \epsilon\sim\mathcal{N}(0,\mathbf{I})$$
### 2.5 Predicting Noise
Since we know $x_t = \sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon_0$ , we can rearrange it and then becomes: $$x_0=\frac{x_t-\sqrt{1-\bar{\alpha}_t}\epsilon_0}{\sqrt{\bar{\alpha}_t}}$$Substituting this into $\mu_q(x_t, x_0)$, it then shows that$$\mu_q(x_t, x_0)=\frac{1}{\sqrt{\alpha_t}}x_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha_t}}\sqrt{\alpha_t}}\epsilon_0$$In other words, we can design $\mu_\theta$ to match the form above, that is:$$\mu_\theta(x_t)=\frac{1}{\sqrt{\alpha_t}}x_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha_t}}\sqrt{\alpha_t}}\hat{\epsilon}_\theta(x_t)$$Then a new ELBO can be derived in the same way, and then the training DDPM algorithm becomes:$$argmin_\theta\mathbb{E}_{x_0,\epsilon_0}ELBO(x_0,\epsilon_0)\approx argmin_\theta\sum_{x_0\sim\mathcal{X}}\frac{1}{M}\sum_{m=1}^MELBO_\theta(x_0,\epsilon_0^{(m)})$$Then the algorithm gradient descent: $\nabla_\theta||\hat{\epsilon}_\theta(x_t)-\epsilon_0||^2$

### 2.6 Denoising Diffusion Implicit Model (DDIM)

From DDPM to DDIM. One of the most prevalent drawbacks of DDPM is that they need a large number of iterations to generate a reasonably good looking image: the reverse diffusion process (denoise process) HAS TO DO GRADIENT DESCENT until CONVERGE

*DDIM deals with this issue by shifting from Markovian to non-Markovian*
Recall that the original transition function:$$q(x_t|x_{t-1})\equiv\mathcal{N}(\sqrt{\alpha_t}x_{t-1},(1-\alpha_t)\mathbf{I})$$$$q(x_t|x_0)\equiv\mathcal{N}(\sqrt{\bar{\alpha}_t}x_0,(1-\bar{\alpha}_t)\mathbf{I})$$Now, we'll design in a new way:$$q(x_t|x_{t-1})\equiv\mathcal{N}(\sqrt{\frac{\alpha_t}{\alpha_{t-1}}}x_{t-1},(1-\frac{\alpha_t}{\alpha_{t-1}})\mathbf{I})$$Which can simplify $\bar{\alpha_t}=\alpha_t$, assume that $\alpha_0=1$$$x_{t-1}=\sqrt{\alpha_{t-1}}x_0+\sqrt{1-\alpha_{t-1}}\epsilon=\sqrt{\alpha_{t-1}}x_0+\sqrt{1-\alpha_{t-1}}\frac{x_t-\sqrt{\alpha_t}x_0}{\sqrt{1-\alpha_t}}$$Where the second equation can be derived from the transition formula of $x_t$.

Recall previously in DDPM we derive $q_\phi(x_{t-1}|x_t, x_0)$ by Bayes theorem: $$q_\phi(x_{t-1}|x_t,x_0)=\frac{q_\phi(x_t|x_{t-1}(,x_0))q_\phi(x_{t-1}|x_0)}{q_\phi(x_t|x_0)}$$And then say that $$x_{t-1}=\mu_q(x_t,x_0)+\sigma_q\epsilon:\mathcal{N}(0,\mathbf{I})$$
But now, from the transition function, we can propose a basic concept that$$q(x_{t-1}|x_t,x_0)=^?\mathcal{N}(\sqrt{\alpha_{t-1}}x_0+\sqrt{1-\alpha_{t-1}}\bigg(\frac{x_t-\sqrt{\alpha_t}x_0}{\sqrt{1-\alpha_t}}\bigg),\sigma_t^2\mathbf{I}):\sigma_t\ is\ a\ hyperparameter$$THE MOST IMPORTANT argument in DDIM is that we want marginal distribution $q(x_{t-1}|x_0)$ to have the same form as $q(x_t|x_0)$, since when we are talking about the marginal distributions, ultimately we want t = 0 it's the original image, and t = T it's a white Gaussian, under this circumstances, there are only some specialized transition probability that allows all marginal distributions have the same form.
And then we finally can modify our pre-proposed transition distribution as:$$q(x_{t-1}|x_t,x_0)=\mathcal{N}(\sqrt{\alpha_{t-1}}x_0+\sqrt{1-\alpha_{t-1}-\sigma_t^2}\bigg(\frac{x_t-\sqrt{\alpha_t}x_0}{\sqrt{1-\alpha_t}}\bigg),\sigma_t^2\mathbf{I}):\sigma_t\ is\ a\ hyperparameter$$By Bishop formula.

#### Inference for DDIM
$$x_t=\sqrt{\alpha_t}x_0+\sqrt{1-\alpha_t}\epsilon\rightarrow x_0=\frac{1}{\sqrt{\alpha_t}}(x_t-\sqrt{1-\alpha_t}\epsilon)$$$$f_\theta^{(t)}(x_t)\equiv\frac{1}{\sqrt{\alpha_t}}\bigg(x_t-\sqrt{1-\alpha_t}\epsilon_\theta^{(t)}(x_t)\bigg)$$Then the inference is just: 


### 3. Score-Matching Langevin Dynamics(SMLD)

#### 3.1 Sampling from a Distribution

Sample: Given a distribution p(x), we want to draw samples x from p(x).$$Sampling:\ x^*=\underset{x}{\arg\max}\ \log p(x;\theta)$$Unlike maximum likelihood:$$\theta^*=\underset{\theta}{argmax}\ \log p(x;\theta)$$In general, it's hard to acquire the analytical solutions for a high-dimensional space with various local minima. Instead, we have to use gradient descent as our method to optimization problem.$$x_{t+1}=x_t+\tau\nabla_x\log p(x_t)$$We use "+" instead of "-" since are now try to maximize instead of minimizing. And here we introduce *Langevin equation*$$x_{t+1}=x_t+\tau\nabla_x\log p(x_t)+\sqrt{2\tau}\mathbf{z},\ z\sim\mathcal{N}(0,\mathbf{I}),\tau:step\ size,x_0:white\ noise$$Notice the participation of noise term in this equation, we can justify the addition of noise by *Fokker-Planck equation*$$\partial_tp(x,t)=-\partial_x\big\{[\partial_x(\log p(x))]p(x,t)\big\}+\partial_x^2p(x,t):p(x)\ is\ the\ ground\ truth\ distribution$$An easy justification for the equation: let lim_{t->\infty} p(x,t) = p(x), then the LHS will be zero

#### 3.2 Stein's Score Function
$$s_\theta(x)\equiv\nabla_x\log p_\theta(x)$$This can represent the trajectory of x_t since x is updated and drifted by the gradient. 

#### 3.3 Score Matching Techniques
Since we have no access to obtain p(x), then we have some techniques to approximate it
##### Explicit Score-Matching
$$q_h(x)=\frac{1}{M}\sum_{m=1}^M\frac{1}{h}K\bigg(\frac{x-x^{(m)}}{h}\bigg)$$An example for K: Gaussian kernel $K(u)=\frac{1}{\sqrt{2\pi}}e^{-u^2/2}$
The loss function is:$$J_{ESM}(\theta)\equiv\frac{1}{2}\mathbb{E}_{p(x)}||s_\theta(x)-\nabla_x\log p(x)||^2\approx\frac{1}{2}\mathbb{E}_{q_h(x)}||s_\theta(x)-\nabla_x\log q_h(x)||^2:h \ is\ hyperparameter\ for\ kernel\ K$$In other words, we want to find a proper theta such that s_theta can approximate the gradient of log q_h. Once the training is completed, we can replace it in the Langevin dynamics equation to obtain the recursion:$$x_{t+1}=x_t+\tau s_\theta(x_t)+\sqrt{2\tau}z$$However, the issue is that the kernel estimation has POOR PERFORMANCE for high dimensional cases.

##### Implicit Score Matching
$$J_{ISM}(\theta)\equiv\mathbb{E}_{p(x)}\bigg[Tr(\nabla_xs_\theta(x))+\frac{1}{2}||s_\theta(x)||^2\bigg]$$However, this way the trace is hard to compute.

##### Denoising Score Matching
$$J_{DSM}(\theta)\equiv\mathbb{E}_{q(x,x')}\bigg[\frac{1}{2}||s_\theta(x)-\nabla_x \log q(x|x')||^2\bigg]$$Special case: where $q(x|x')=\mathcal{N}(x|x',\sigma^2)$, then $x=x'+\sigma z$. Then we can calculate:$$J_{DSM}(\theta)=\mathbb{E}_{q(x')}\bigg[\frac{1}{2}||s_\theta(x'+\sigma z)+\frac{z}{\sigma}||^2\bigg]:z\sim\mathcal{N}(0,\mathbf{I})$$And then replace x' with x: $$J_{DSM}(\theta)=\mathbb{E}_{p(x)}\bigg[\frac{1}{2}||s_\theta(x+\sigma z)+\frac{z}{\sigma}||^2\bigg]$$It's a denoising process: x+noise as input, and then the score function aims to predict noise z/sigma.

##### Equivalence
$$J_{DSM}(\theta)=J_{ESM}(\theta)+C$$The loss function of DSM and ESM only differs a constant, which implies that their minimizers are the same.
Training process: $$\theta^*=\underset{\theta}{argmax}\mathbb{E}_{p(x)}\big[\frac{1}{2}||s_\theta(x+\sigma z)+\frac{z}{\sigma}||^2\big]\approx\underset{\theta}{argmin}\frac{1}{M}\sum_{m=1}^M\frac{1}{2}||s_\theta(x^{(m)}+\sigma z^{(m)})+\frac{z^{(m)}}{\sigma}||^2:z^{(m)}\sim\mathcal{N}(0,\mathbf{I})$$
##### Noise Conditioned Score Network (NCSN)
The above procedure is using a fixed noise level $\sigma$. We now want to generalize it to multiple noise levels:$$J_{NCSN}(\theta)=\frac{1}{L}\sum_{i=1}^L\lambda(\sigma_i)l(\theta;\sigma_i)\ where\ \ l(\theta;\sigma_i)=\mathbb{E}_{p(x)}\bigg[\frac{1}{2}||s_\theta(x+\sigma_i z)+\frac{z}{\sigma_i}||^2\bigg]$$And $\lambda(\sigma_i)=\sigma_i^2$ always based on experiments. The noise sequence often satisfies $\frac{\sigma_1}{\sigma_2}=\dots=\frac{\sigma_{L-1}}{\sigma_L}>1$

For inference: $$x_{t+1}=x_t+\frac{\alpha_i}{2}s_\theta(x_t,\sigma_i)+\sqrt{\alpha_i}z_t:\alpha_i=\sigma_i^2/\sigma_L^2$$The iteration over t is repeated sequentially for each sigma_i from i = 1 to L.

### 4. Stochastic Differential Equation SDE
#### 4.1 From Iterative Algorithms to Differential Equations
The iterative relation of x in different steady states (t) can be described by differential equations, which has analytical solution allowing us to find it instead of doing a lot of iterations.

Take Gradient Descent as example:$$x_i-x_{i-1}=-\beta_{i-1}\nabla f(x_{i-1}) \Rightarrow dx=-\beta\nabla f(x)dt:assume\ \beta(t)=\beta$$The forward equation: $$x_i=x_{i-1}+\Delta x_{i-1}\approx x_{i-1}+dx \approx x_{i-1}-\beta_{i-1}\nabla f(x_{i-1})$$The backward equation:$$x_{i-1}=x_i-\Delta x_i\approx x_i + \beta_i\nabla f(x_i)$$
#### 4.2 Introduction of SDE
$$\frac{dx(t)}{dt}=f(t,x)+g(t,x)\xi(t),\ where\ \xi(t)\sim\mathcal{N}(0,\mathbf{I})$$We can then rewrite as: $$dx(t)=f(t,x)dt+g(t,x)\xi(t)dt=f(t,x)dt+g(t,x)dw:dw\ is\ Brownian\ motion$$Because xi is random, then the solution is random as well. To explicitly obtain the solution, we should interpret the differential as follows and also have to introduce $\omega$ which corresponds to a distinct random path of $\xi$ for practical realization:$$x(t,\omega)=x_0+\int_0^tf(s,x(s,\omega))dt+\int_0^tg(s,x(s,\omega))dw(s,\omega)$$My own realization: $dw(s,\omega)=\xi(s,\omega)ds=\xi(s)ds$
Where $\xi(s)$ isn't random anymore!
#### 4.3 Stochastic Differential Equation for DDPM and SMLD

Forward Diffusion Process:$$dx=f(x,t)dt+g(t)d\mathbf{w}$$Where f(x,t) is for drifting, and g(t) is for diffusion, $dw = \xi(t)dt$ is account for the forward Wiener process

Reverse Diffusion Process: $$dx=[f(x,t)-g(t)^2\nabla_x\log p_t(x)]dt + g(t)d\bar{w}$$Where we notice the score function is involved, and $d\bar{w}$ is account for the reverse Wiener process.
We can easily realize that $\bar{w}(T)=w(0)$ and $\bar{w}(0)=w(T)$. 

*VP=variation preserving, VE=variation exploding*
##### Stochastic Differential Equation for DDPM (VP SDE)
$$x_i=\sqrt{1-\beta_i}x_{i-1}+\sqrt{\beta_i}z_{i-1}:z_{i-1}\sim\mathcal{N}(0,\mathbf{I}),\ i=1,2,\dots,N$$which is equivalent to:$$dx=-\frac{\beta(t)}{2}xdt+\sqrt{\beta(t)}dw$$We can prove the above DE by let $\Delta t=\frac{1}{N}$, $\beta_i=\frac{\bar{\beta}_i}{N}$, then $\beta_i=\beta(\frac{i}{N})\cdot\frac{1}{N}=\beta(t+\Delta t)\Delta t$, where $$N\rightarrow\infty,\bar{\beta}_i\rightarrow\beta(t),0\leq t\leq 1$$Similarly, $x_i=x(\frac{i}{N})=x(t+\Delta t),z_i=z(\frac{i}{N})=z(t+\Delta t)$. As $\Delta t\rightarrow 0$, Q.E.D.

Therefore, for DDPM, $f(t)=-\frac{\beta(t)}{2}$ and $g(t)=\sqrt{\beta(t)}$ 
Then, follow the above-mentioned reverse equation: $$Reverse\ Sampling:dx=-\beta(t)\big[\frac{x}{2}+\nabla_x\log p_t(x)\big]dt+\sqrt{\beta(t)}d\bar{w}$$Which can also be used to derive iterative update scheme by letting $$dx=x(t)-x(t-\Delta t),d\bar{w}=w(t-\Delta t)-w(t)=-z(t), dt=\Delta t$$ and get $$x_{i-1}\approx \frac{1}{\sqrt{1-\beta_i}}\bigg[x_i+\frac{\beta_i}{2}\nabla_x\log p_i(x_i)\bigg]+\sqrt{\beta_i}z_i$$For practical implementation, we can replace gradient of log p with estimated score function $s_\theta(x_i)$.

##### Stochastic Differential Equation for SMLD (VE SDE)

If we divide the noise scale in the SMLD training into N levels, then the recursion: $$x_i=x_{i-1}+\sqrt{\sigma_i^2-\sigma_{i-1}^2}z_{i-1},\ i=1,2,\dots,N$$Then the SDE can be written as$$dx=\sqrt{\frac{d[\sigma(t)^2]}{dt}}dw$$As for the reverse sampling: $$dx=-\bigg(\frac{d[\sigma(t)^2]}{dt}\nabla_x\log p_t(x)\bigg)dt+\sqrt{\frac{d[\sigma(t)^2]}{dt}}d\bar{w}$$The reverse equation can always be derived by identifying f and g in the forward process.
We can justify this by convert the differential equation into iterative steps:$$x_{i-1}=x_i+(\sigma_i^2-\sigma^2_{i-1})\nabla_x\log p_i(x_i)+\sqrt{(\sigma^2_i-\sigma^2_{i-1})}z_i$$which is identical to the SMLD reverse update equation. 

#### 4.4 Numerical Solvers for ODE and SDE
##### Euler Method
First order numerical method for solving ODE:$$\frac{dx(t)}{dt}=f(t,x(t))$$The solution: $$x_{i+1}=x_i+\alpha\cdot f(t_i,x_i),\ i=0,1,\dots,N-1$$Where alpha is the step size.
##### Runge-Kutta(RK) Method
$$x_{i+1}=x_i+\frac{\alpha}{6}\cdot(k_1+2k_2+2k_3+k_4),\ i=1,2,\dots,N$$$k_1=f(x_i,t_i)$, $k_2=f(t_i+\alpha/2,x_i+\alpha k_1/2)$, $k_3=f(t_i+\alpha/2,x_i+\alpha k_2/2)$, $k_4=f(t_i+\alpha,x_i+\alpha k_3)$
##### Predictor Corrector Algorithm
In DDPM: $$x_{i-1}\approx \frac{1}{\sqrt{1-\beta_i}}\bigg[x_i+\frac{\beta_i}{2}\nabla_x\log p_i(x_i)\bigg]+\sqrt{\beta_i}z_i$$
