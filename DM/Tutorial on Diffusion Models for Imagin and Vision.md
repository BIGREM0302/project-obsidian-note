[[Diffusion Model Basic]]
[[KL Divergence]]

This note avoid complex math derivation for simplicity and quick overview
## 1 Variational Auto-Encoder

### 1.1 Building blocks of VAE
Aim: We want to build a generator that generates images from some inputs.

Building block: Input x -> encoder -> latent variable z -> decoder -> generated x

where the latent variable z should extract some valuable information from the input x, and at the same time serves as the seed to generate something $\sim$ x

There are a lot of distributions involved, we should clarify them and figure out what is "known" and what is "unknown", what is to be trained.

- $p(x)$ : The *true* distribution of x, and is NEVER KNOWN.
- $p(z)$ : Typically we *made* it a zero mean unit variance Gaussian "for mathematical convenience"
- $p(z|x)$ : conditional distribution associated with encoder, but we can NEVER ACESS
- $p(x|z)$ : conditional distribution associated with decoder, but we can NEVER ACESS

To find encoder and decoder, we have to determine:
- $q_\phi(z|x)$ : Decoder, can be parameterized using deep NN, and can be any directed graphical model, to approximate p(z|x)
- $p_\theta(x|z)$ : Encoder, to approximate p(x|z)
### 1.2 Evidence Lower Bound

We'd like to find the best theta and phi, so we need an objective function(loss function):$$ELBO(x)\equiv\mathbb{E}_{q_\phi(z|x)}[\log \frac{p(x, z)}{q_\phi(z|x)}]$$Meaning: given x,  the expectatation value of log(p(x, z)/q_phi(z|x)) subject to z ~ q_phi(z|x)
which is the LOWER BOUND for the distribution logp(x) $$\log p(x)=ELBO(x)+KL(q_\phi(z|x)||p(z|x))$$Remember that we want to approximate p(z|x) by using q_phi(z|x)? That is, since logp(x) is a constant (which is generated by the real word, or the universe), then by maximizing ELBO, we can minimize KL divergence between q_phi(z|x) and p(z|x) as the result.
Further we can divide ELBO to the following.$$ELBO(x)=\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]-KL(q_\phi(z|x)||p(z))$$We want to maximize ELBO, so maximize logp_theta, and minimize the difference between p(z) and q_phi(z|x). We can further recognize these two terms as:
- **Reconstruction**: first term is about the decoder, since we are maximize the expectation of logp_theta (the encoding process) subject to z given x. That is, subject to q_theta(encode process) we'd like to increase the likelihood of the reconstruction toward the original x.
- **Prior Matching**: We want the encoder to turn x into latent vector z s.t. it will follow "our choice of distribution" e.g. z~N(0, I)
### 1.3 Optimization in VAE
$$(\phi,\theta) = argmax_{\phi,\theta}\sum_{x\in\mathcal{X}}ELBO(x);\ where\ \mathcal{X}=\{x^{(l)}|l=1,\dots,L\}$$X is the training set with size L. However we find that it's hard to solve if we directly calculate gradient:$$\nabla_\theta ELBO(x)\ and\ \nabla_\phi ELBO(x)$$Even though we can write: *note that the numerator of ELBO here is p_theta instead of real p*
$$\nabla_\theta ELBO(x)\approx \frac{1}{L}\sum_{l=1}^{L}\nabla_\theta\{\log p_\theta(x,z^{(l)})\} :where\ z^{(l)}\sim q_\phi(z|x)$$The approximation is correlated with [[Monte Carlo Approximation]]. Since p_theta is realized by a computable model such as NN, the gradient can be thus computed via automatic differntiation and update the parameter by back propogating. However, we fail to derive the similar representation for parameter phi, the REASON is that we have to draw samples z from distribution q_phi(z|x) which is also dependent of phi!
#### Reparameterization Trick
Aim to tackle with the intractability of ELBO's gradient. i.e. to express z in the following way$$z=g(\epsilon,\phi,x)$$for the invertible and differentiable transformation g and another random varaible epsilon whose distribution is INDEPENDENT of x and phi.

An addtional requirement: $$q_\phi(z|x)\cdot |det(\frac{\partial z}{\partial \epsilon})|=p(\epsilon)\text{ where }\frac{\partial z}{\partial \epsilon}:=
\begin{bmatrix}
\frac{\partial z_1}{\partial \epsilon_1}&\dots&\frac{\partial z_1}{\partial \epsilon_n}\\
\vdots & & \vdots\\
\frac{\partial z_n}{\partial \epsilon_1}&\dots&\frac{\partial z_n}{\partial \epsilon_n}
\end{bmatrix}:Jacobian\ matrix$$With this re-parameterization of z, we can write:$$\mathbb{E}_{q_\phi(z|x)}[f(z)]=\dots=\mathbb{E}_{p(\epsilon)}[f(z)]$$and therefore we can further calculate the gradient:$$\nabla_\phi ELBO(x)\approx\frac{1}{L}\sum_{l=1}^L\nabla_\phi[\log|det\frac{\partial z^{(l)}}{\partial \epsilon^{(l)}}|]$$Similarly, the approximation is correlated to Monte Carlo approximation. Now both gradients we can calculate thanks to this trick and also the approximation, which can lead to unbiased estimate when maximizing ELBO.
#### VAE Encoder
ASSUME a COMMON CHOICE of encoder: The Gaussian Distribution$$q_\phi(z|x)=\mathcal{N}(z|\mu_\phi(x),\sigma_\phi(x)^2I)$$Then we can use the neural network to determine the mean and variance of the Gaussian given input, and then sample latent z from the distribution q_phi. A more convenient way to express z is to adopt the REPARAMETERIZATION trick:$$z^{(l)}|x^{(l)}=\mu_\phi(x^{(l)})+\sigma_\phi(x^{(l)})\epsilon,\ \epsilon\sim\mathcal{N}(0,I)$$Notice that latent variable z is still p(z) = N(0,I). The epsilon is also a gaussian, it's consistent with the additional requirement elaborated above: $q_\phi(z|x)\cdot |det(\frac{\partial z}{\partial \epsilon})|=p(\epsilon)$ 
Let's substitute the q_phi derived above into the prior matching term of ELBO: $$\nabla\phi\bigg( KL\big(q_\phi(z|x)||p(z)\big)\bigg)=\nabla_\phi\bigg(something\ related\ to\ \phi\ instead\ of\ \theta\bigg)$$It's derived by using the formula for KL divergence for two gaussian distributions, and a closed form of the gradient can be numerically solved. 

#### VAE Decoder
ASSUME a COMMON CHOICE of decoder: The Gaussisan Distribution$$p_\theta(x|z)=\mathcal{N}(x|f_\theta(z),\sigma_{dec}^2I):\sigma_{dec}\text{ are some hyperparameter}$$Estimate the mean of Guassisan by input latenze vector z to a NN. Similarly, by reparameterization trick the generated x can be written as $\hat{x}=f_\theta(z)+\sigma_{dec}\epsilon,\ \epsilon\sim\mathcal{N}(0,I)$. 
Then, we can now calculate the Reconstruction term of ELBO:$$\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]=\int \dots\approx-\frac{1}{M}\sum_{m=1}^{M}\frac{||x-f_\theta(z^{(m)})||^2}{2\sigma^2_{dec}}=-\frac{1}{M}\sum_{m=1}^{M}\frac{||x-f_\theta\big(\mu_\phi(x)+\sigma_\phi(x)\epsilon^{(m)}\big)||^2}{2\sigma^2_{dec}}$$The approximation is again from Monte Carlo. Note that M is the number of Monte Carlo samples we want to use to estimate the expectation, where the randomness is achieved by $\epsilon\sim\mathcal{N}(0.I)$
Note x is fixed since the whole equation is under a given x!

Gradient of this term respect to theta is easily to compute since we know f_theta.. Respect to phi can also be calculated by chain rule. We can still use gradient descent to train NN.

### 1.4 Conclusion
To train VAE = to optimize theta and phi by runing *stochastic* gradient descent$$argmax_{\theta,\phi}ELBO_{\phi,\theta}(x)$$$$ELBO_{\phi,\theta}=-\frac{1}{M}\sum_{m=1}^{M}\frac{||x-f_\theta\big(\mu_\phi(x)+\sigma_\phi(x)\epsilon^{(m)}\big)||^2}{2\sigma^2_{dec}}+\frac{1}{2}\bigg(\sigma^2_\phi(x)d-d+||\mu_\phi(x)||^2-2d\log\sigma_\phi(x)\bigg)$$
where d is the dimension of latent variable z

VAE Inference = generate image by randomly pick $z\in\mathbb{R}^d$ , send z through decoder f_theta and then generate x = f_theta(z)

## 2. DDPM